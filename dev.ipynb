{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df167198",
   "metadata": {},
   "source": [
    "#### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719b32e8",
   "metadata": {},
   "source": [
    "##### LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df6ff03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f88f67e",
   "metadata": {},
   "source": [
    "##### Chamando a LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cd5b3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Alice sempre foi fascinada pelo mundo da tecnologia e sempre sonhou em criar suas próprias ferramentas e aplicativos. No entanto, ela nunca teve a oportunidade de aprender a programar, pois cresceu em uma família onde essa área não era incentivada.\n",
      "\n",
      "Mas um dia, Alice decidiu que era hora de seguir seu sonho e começar sua jornada de aprendizado em programação. Ela se matriculou em um curso online e começou a estudar todos os dias após o trabalho. No início, ela se sentia um pouco perdida e confusa com todos os códigos e termos técnicos, mas ela estava determinada a não desistir.\n",
      "\n",
      "Com o tempo, Alice começou a entender os conceitos básicos de programação e foi capaz de criar seu primeiro programa simples. Ela ficou tão animada com o resultado que decidiu continuar aprendendo e se inscreveu em workshops e palestras sobre programação.\n",
      "\n",
      "Com o passar dos meses, Alice se tornou mais confiante e habilidosa em programação. Ela começou a se desafiar com projetos mais complexos e, mesmo quando enfrentava dific"
     ]
    }
   ],
   "source": [
    "pergunta = 'Conte uma história breve sobre a jornada de aprender a programar'\n",
    "for trecho in llm.stream(pergunta):\n",
    "    print(trecho, end='')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c923c08c",
   "metadata": {},
   "source": [
    "##### Chamadas simultaneas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff69894b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\nO céu é uma região do espaço que fica acima da atmosfera terrestre, composta por gases raros, como o oxigênio e o nitrogênio, e outros elementos, como poeira cósmica e água. É onde se encontram as estrelas, planetas, asteroides e outras galáxias. É também o local onde podemos observar fenômenos astronômicos, como a lua, os planetas e as constelações. Para muitas culturas, o céu é considerado um lugar sagrado, associado a divindades e à vida após a morte.',\n",
       " '\\n\\nA terra é o terceiro planeta do sistema solar, localizado a uma distância média de 149,6 milhões de quilômetros do sol. É o único planeta conhecido até o momento que possui condições adequadas para a existência de vida, como água líquida, oxigênio e uma atmosfera protetora. É composta por camadas geológicas, incluindo a crosta, o manto e o núcleo, e abriga uma grande diversidade de seres vivos, incluindo humanos. Além disso, a terra é o lar de diversas paisagens, como montanhas, oceanos, florestas e desertos, e apresenta uma grande variedade de climas e condições ambientais.',\n",
       " '\\n\\nAs estrelas são grandes corpos celestes que emitem luz e calor próprios, formados principalmente por hidrogênio e hélio. Elas são sustentadas pela fusão nuclear em seu núcleo, onde ocorre a transformação de elementos mais leves em elementos mais pesados, liberando uma grande quantidade de energia. Existem bilhões de estrelas no universo, cada uma com características únicas, como tamanho, temperatura, brilho e cor. Elas desempenham um papel fundamental na formação e evolução das galáxias e sistemas planetários. Na Terra, a estrela mais próxima é o Sol, fonte de luz e calor que permite a existência de vida em nosso planeta. ']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perguntas = [\n",
    "    'O que é o céu?',\n",
    "    'O que é a terra?',\n",
    "    'O que são as estrelas?'\n",
    "]\n",
    "\n",
    "llm.batch(perguntas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05b3827",
   "metadata": {},
   "source": [
    "#### Chat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "465a7439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(model = 'gpt-3.5-turbo-0125')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b46b2f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "mensagens = [\n",
    "    SystemMessage(content= 'Você é um assistente que conta piadas.'),\n",
    "    HumanMessage(content='Quanto é 1 + 1?')\n",
    "]\n",
    "\n",
    "\n",
    "resposta = chat.invoke(mensagens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "815d55ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 18,\n",
       "  'prompt_tokens': 30,\n",
       "  'total_tokens': 48,\n",
       "  'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "   'audio_tokens': 0,\n",
       "   'reasoning_tokens': 0,\n",
       "   'rejected_prediction_tokens': 0},\n",
       "  'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
       " 'model_name': 'gpt-3.5-turbo-0125',\n",
       " 'system_fingerprint': None,\n",
       " 'finish_reason': 'stop',\n",
       " 'logprobs': None}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resposta.response_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0686646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depende, você está pedindo a resposta matemática ou uma piada?"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "mensagens = [\n",
    "    SystemMessage(content= 'Você é um assistente que conta piadas.'),\n",
    "    HumanMessage(content='Quanto é 1 + 1?')\n",
    "]\n",
    "\n",
    "for trecho in chat.stream(mensagens):\n",
    "    print(trecho.content, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf8d374",
   "metadata": {},
   "source": [
    "#### Conceitos avançados de Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9f4528",
   "metadata": {},
   "source": [
    "##### Prompt few-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5398623e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f100ccdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='A soma de 10 + 4 é igual a 14.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 52, 'total_tokens': 66, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-0561c8bb-126d-4769-bc4f-7e2116fdd763-0', usage_metadata={'input_tokens': 52, 'output_tokens': 14, 'total_tokens': 66, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "mensagens = [\n",
    "    HumanMessage(content='Quanto é 1 + 1?'),\n",
    "    AIMessage(content='2'),\n",
    "    HumanMessage(content='Quanto é 10 * 5?'),\n",
    "    AIMessage(content='50'),\n",
    "    HumanMessage(content='Quanto é 10 + 4?')\n",
    "\n",
    "]\n",
    "\n",
    "chat.invoke(mensagens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109a6650",
   "metadata": {},
   "source": [
    "#### Utilizando outros modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053cd6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models.huggingface import ChatHuggingFace\n",
    "from langchain_community.llms.huggingface_endpoint import HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a55d305",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Incorrect path_or_model_id: 'mistralai/Mistral-7B-Instruct-v0.2:featherless-ai'. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Thiago\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Thiago\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 106\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Thiago\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:160\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[1;34m(repo_id)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n\u001b[1;32m--> 160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must use alphanumeric chars or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m forbidden, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot start or end the name, max length is 96:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    164\u001b[0m     )\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id:\n",
      "\u001b[1;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: 'mistralai/Mistral-7B-Instruct-v0.2:featherless-ai'.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m modelo \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai/Mistral-7B-Instruct-v0.2:featherless-ai\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m llm \u001b[38;5;241m=\u001b[39m HuggingFaceEndpoint(repo_id\u001b[38;5;241m=\u001b[39mmodelo)\n\u001b[1;32m----> 4\u001b[0m chat \u001b[38;5;241m=\u001b[39m \u001b[43mChatHuggingFace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Thiago\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:224\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    222\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     emit_warning()\n\u001b[1;32m--> 224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Thiago\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_community\\chat_models\\huggingface.py:75\u001b[0m, in \u001b[0;36mChatHuggingFace.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resolve_model_id()\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m---> 75\u001b[0m     \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\n\u001b[0;32m     78\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Thiago\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:858\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    857\u001b[0m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[1;32m--> 858\u001b[0m tokenizer_config \u001b[38;5;241m=\u001b[39m \u001b[43mget_tokenizer_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[0;32m    860\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Thiago\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:690\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[1;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[0;32m    687\u001b[0m     token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[0;32m    689\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 690\u001b[0m resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    707\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Thiago\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\hub.py:469\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was a specific connection error when trying to load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HFValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 469\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    470\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect path_or_model_id: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    471\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n",
      "\u001b[1;31mOSError\u001b[0m: Incorrect path_or_model_id: 'mistralai/Mistral-7B-Instruct-v0.2:featherless-ai'. Please provide either the path to a local folder or the repo_id of a model on the Hub."
     ]
    }
   ],
   "source": [
    "modelo = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "llm = HuggingFaceEndpoint(repo_id=modelo)\n",
    "chat = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be5a432e",
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: Root=1-68b841f6-41bc32bb3b989e7544ceb5ef;4c8a7aef-70bd-40bd-8618-83ccd3da70eb)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Thiago\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 12\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessages\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HumanMessage, AIMessage\n\u001b[0;32m      3\u001b[0m mensagens \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      4\u001b[0m     HumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuanto é 1 + 1?\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m      5\u001b[0m     AIMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m \n\u001b[0;32m     10\u001b[0m ]\n\u001b[1;32m---> 12\u001b[0m \u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmensagens\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Thiago\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:369\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    365\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    366\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m--> 369\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    376\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    379\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32mc:\\Users\\Thiago\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:946\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    937\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    939\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    943\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    944\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    945\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 946\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Thiago\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:765\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[0;32m    763\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    764\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 765\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    766\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    767\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    768\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    769\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    770\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    771\u001b[0m         )\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    773\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\Thiago\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1011\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1009\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1011\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1015\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Thiago\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_community\\chat_models\\huggingface.py:137\u001b[0m, in \u001b[0;36mChatHuggingFace._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generate_from_stream(stream_iter)\n\u001b[0;32m    136\u001b[0m llm_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_chat_prompt(messages)\n\u001b[1;32m--> 137\u001b[0m llm_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mllm_input\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_chat_result(llm_result)\n",
      "File \u001b[1;32mc:\\Users\\Thiago\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:1545\u001b[0m, in \u001b[0;36mLLM._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1542\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m   1544\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1545\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1546\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m   1547\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1548\u001b[0m     )\n\u001b[0;32m   1549\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[0;32m   1550\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32mc:\\Users\\Thiago\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_community\\llms\\huggingface_endpoint.py:267\u001b[0m, in \u001b[0;36mHuggingFaceEndpoint._call\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    264\u001b[0m     invocation_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m invocation_params[\n\u001b[0;32m    265\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    266\u001b[0m     ]  \u001b[38;5;66;03m# porting 'stop_sequences' into the 'stop' argument\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minvocation_params\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    273\u001b[0m         response_text \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mdecode())[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Thiago\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:306\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[1;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 306\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32mc:\\Users\\Thiago\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:477\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[1;32m--> 477\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: Root=1-68b841f6-41bc32bb3b989e7544ceb5ef;4c8a7aef-70bd-40bd-8618-83ccd3da70eb)"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "mensagens = [\n",
    "    HumanMessage(content='Quanto é 1 + 1?'),\n",
    "    AIMessage(content='2'),\n",
    "    HumanMessage(content='Quanto é 10 * 5?'),\n",
    "    AIMessage(content='50'),\n",
    "    HumanMessage(content='Quanto é 10 + 4?')\n",
    "\n",
    "]\n",
    "\n",
    "chat.invoke(mensagens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f9cb9a",
   "metadata": {},
   "source": [
    "##### A estrutura de chat_model utiliza a estrutura de llm como backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cb4a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "\n",
    "langchain.debug = True\n",
    "chat.invoke(mensagens)\n",
    "langchain.debug = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd62ca00",
   "metadata": {},
   "source": [
    "#### Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc3e2d9",
   "metadata": {},
   "source": [
    "##### Cache em memória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82d1054",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(model='gpt-3.5-turbo-0125')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216e7cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "mensagens = [\n",
    "    SystemMessage(content='Você é um assistente engraçado.'),\n",
    "    HumanMessage(content='Quanto é 1 + 1?')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d2e807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.cache import InMemoryCache\n",
    "from langchain.globals import set_llm_cache\n",
    "\n",
    "set_llm_cache(InMemoryCache())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5a53b6",
   "metadata": {},
   "source": [
    "###### Rodando a primeira vez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f274217",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "chat.invoke(mensagens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46866340",
   "metadata": {},
   "source": [
    "###### Rodando novamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc51717",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "chat.invoke(mensagens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932c2036",
   "metadata": {},
   "source": [
    "##### Cache SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2de01fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.cache import SQLiteCache\n",
    "from langchain.globals import set_llm_cache\n",
    "\n",
    "set_llm_cache(SQLiteCache(database_path=fr'C:\\Users\\Thiago\\Desktop\\projeto_langchain\\arquivos\\langchain_cache_db.sqlite'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ad9f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "chat.invoke(mensagens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb553c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "chat.invoke(mensagens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e5c57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "chat.invoke(mensagens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713eab24",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "chat.invoke(mensagens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e955d866",
   "metadata": {},
   "source": [
    "#### Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bc089c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.llms import OpenAI\n",
    "\n",
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfb4e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template('''\n",
    "Responda a seguinte pergunta do usuário:\n",
    "{pergunta}\n",
    "''')\n",
    "\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a06a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_template.format(pergunta='O que é um buraco negro?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4f8c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template('''\n",
    "Responda a seguinte pergunta do usuário em até {n_palavras} palavras:\n",
    "{pergunta}\n",
    "''')\n",
    "\n",
    "prompt_template.format(n_palavras=10, pergunta='O que é um buraco negro?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f184272",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate.from_template('''\n",
    "Responda a seguinte pergunta do usuário em até {n_palavras} palavras:\n",
    "{pergunta}\n",
    "''', partial_variables={'n_palavras': 10}) # valor default\n",
    "\n",
    "prompt_template.format(pergunta='O que é um buraco negro?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff31947",
   "metadata": {},
   "source": [
    "##### Composing prompts | Unindo multiplos prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd7e151",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template_word_count = PromptTemplate.from_template('''\n",
    "Responda a pergunta em até {n_palavras} palavras.\n",
    "''')\n",
    "\n",
    "template_lingua = PromptTemplate.from_template('''\n",
    "Retorn a resposta em {lingua}.\n",
    "''')\n",
    "\n",
    "template_final = (\n",
    "    template_word_count\n",
    "    + template_lingua\n",
    "    + 'Responda a pergunta seguinte seguindo as instruções: {pergunta}'\n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "template_final.template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b078f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = template_final.format(n_palavras=10, lingua='Inglês', pergunta='O que é uma estrela?')\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d9a4ea",
   "metadata": {},
   "source": [
    "##### Templates para chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028a7d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_template('Essa é a minha dúvida: {duvida}')\n",
    "chat_template.format_messages(duvida='Quem sou eu?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4d829b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', 'Você é um assistente engraçado e se chama {nome_assistente}'),\n",
    "        ('human', 'Olá, como vai?'),\n",
    "        ('ai', 'Melhor agora! Como posso ajudá-lo?'),\n",
    "        ('human', '{pergunta}')\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef287476",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template.format_messages(nome_assistente='Asimo', pergunta='Qual o seu nome?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aee76ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "chat.invoke(chat_template.format_messages(nome_assistente='Asimo', pergunta='Qual o seu nome?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93599833",
   "metadata": {},
   "source": [
    "##### Templates de few-shot prompting para llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051a7681",
   "metadata": {},
   "outputs": [],
   "source": [
    "exemplos = [\n",
    "\n",
    "    {\"pergunta\": \"Quem viveu mais tempo, Muhammed Ali ou Alan Turing?\",\n",
    "     \"resposta\": \"\"\"\n",
    "São necessárias perguntas de acompanhamento aqui: Sim,\n",
    "Pergunta de acompanhamento: Quantos anos Muhammad Ali tinha quando morreu?\n",
    "Resposta intermediária: Muhammad Ali tinha 74 anos quando morreu.\n",
    "Pergunta de acompanhamento: Quantos anos Alan Turing tinha quando morreu?\n",
    "Resposta intermediária: Alan Turing tinha 41 anos quando morreu.\n",
    "Então a resposta final é: Muhammad Ali.\n",
    "\"\"\",\n",
    "     },\n",
    "\n",
    "     {\"pergunta\": \"Quando nasceu o fundador do craigslist?\",\n",
    "     \"resposta\": \"\"\"\n",
    "São necessárias perguntas de acompanhamento aqui: Sim,\n",
    "Pergunta de acompanhamento: Quem foi o fundador do craigslist?\n",
    "Resposta intermediária: O craigslist foi fundado por Craig Newmark.\n",
    "Pergunta de acompanhamento: Quando nasceu Craig Newmark?\n",
    "Resposta intermediária: Craig Newmark nasceu em 6 de dezembro de 1952.\n",
    "Então a resposta final é: 6 de dezembro de 1952.\n",
    "\"\"\",\n",
    "     },\n",
    "\n",
    "     {\"pergunta\": \"Quem foi o avô materno de George Washington?\",\n",
    "     \"resposta\": \"\"\"\n",
    "São necessárias perguntas de acompanhamento aqui: Sim,\n",
    "Pergunta de acompanhamento: Quem foi a mãe de George Washington?\n",
    "Resposta intermediária: A mãe de George Washington foi Mary Ball Washington.\n",
    "Pergunta de acompanhamento: Quem foi o pai de Mary Ball Washington?\n",
    "Resposta intermediária: O pai de Mary Ball Washington foi Joseph Ball.\n",
    "Então a resposta final é: Joseph Ball.\n",
    "\"\"\",\n",
    "     }\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfed5084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=['pergunta', 'resposta'],\n",
    "    template='Pergunta {pergunta} \\n{resposta}'\n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "example_prompt.format(**exemplos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24208317",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = FewShotPromptTemplate(\n",
    "    examples=exemplos,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix='Pergunta: {input}',\n",
    "    input_variables=['input']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaf1f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bda9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt.format(input='Quem fez mais gols, Romário ou Pelé?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2339806",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(prompt.format(input='Quem fez mais gols, Romário ou Pelé?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763f8867",
   "metadata": {},
   "source": [
    "##### Templates de Few-shot prompting para chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66cf570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e05d126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [('human', '{pergunta}'),\n",
    "     ('ai', '{resposta}')]\n",
    ")\n",
    "print(example_prompt.format_messages(**exemplos[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afe66ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_templates = FewShotChatMessagePromptTemplate(\n",
    "    examples=exemplos,\n",
    "    example_prompt=example_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33009a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "    few_shot_templates, \n",
    "    ('human', '{input}')\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt = prompt_template.format_messages(input='Quem fez mais gols, Romário ou Pelé?')\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763f9e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805310af",
   "metadata": {},
   "source": [
    "#### Output Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c7b28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', 'Você é um assistente engraçado e se chama {nome_assistente}'),\n",
    "        ('human', '{pergunta}')\n",
    "    \n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_template.format_messages(nome_assistente = 'Asimo', pergunta='Qual o seu nome?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82d8834",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = chat_template.invoke({'nome_assistente': 'Asimo', 'pergunta': 'Qual o seu nome?'})\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243e4a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "resposta = chat.invoke(prompt)\n",
    "resposta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c5268e",
   "metadata": {},
   "source": [
    "##### StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f0a627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "output_parser.invoke(resposta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f529d7af",
   "metadata": {},
   "source": [
    "##### Dando um spoiler de chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5eb81ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chat_template | chat | output_parser\n",
    "\n",
    "chain.invoke({'nome_assistente': 'Asimo', 'pergunta': 'Qual o seu nome?'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f824fd6",
   "metadata": {},
   "source": [
    "##### Utilizando .with_structured_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b047e9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaec1995",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Piada(BaseModel):\n",
    "    \"\"\"Piada para contar ao usuário\"\"\"\n",
    "    introducao: str = Field(description='A introdução da piada')\n",
    "    punchline: str = Field(description='A conclusão da piada')\n",
    "    avaliacao: Optional[int] = Field(description='O quão engraçada é a piada de 1 a 10?')\n",
    "\n",
    "\n",
    "llm_estrturada = chat.with_structured_output(Piada)\n",
    "resposta = llm_estrturada.invoke('Conte uma piada sobre gatinhos')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b119b6d1",
   "metadata": {},
   "source": [
    "##### Um exemplo mais prático"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37bbdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_cliente = \"\"\"\n",
    "Este soprador de folhas é bastante incrível. Ele tem quatro configurações: sopro de vela, brisa suave, cidade ventosa e tornado. \n",
    "Chegou em dois dias, bem a tempo para o presente de aniversário da minha esposa. Acho que minha esposa gostou tanto que ficou sem palavras. \n",
    "Até agora, fui o unico a usá-lo, e tenho usado em todas as manhãs alternadas para limpar as folhas do nosso gramado. É um pouco mais caro do que os outros sopradores de folhas \n",
    "disponíveis no mercado, mas acho que vale a pena pelas características extras. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867cb292",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class AvaliacaoReview(BaseModel):\n",
    "    \"\"\"Avalia review do cliente\"\"\"\n",
    "    presente: str = Field(description='Verdadeiro se foi para presente e False se não foi')\n",
    "    dias_entrega: str = Field(description='Quantos dias para entrega do produto')\n",
    "    percepcao_valor: Optional[int] = Field(description='Extraia qualquer frase sobre o valor ou preço do produto. Retorne uma lista.')\n",
    "\n",
    "\n",
    "llm_estrturada = chat.with_structured_output(AvaliacaoReview)\n",
    "resposta = llm_estrturada.invoke(review_cliente)\n",
    "resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d8325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "# 1️⃣ Define o modelo Pydantic\n",
    "class AvaliacaoReview(BaseModel):\n",
    "    \"\"\"Avalia review do cliente\"\"\"\n",
    "    presente: str = Field(description='Verdadeiro se foi para presente e False se não foi')\n",
    "    dias_entrega: str = Field(description='Quantos dias para entrega do produto')\n",
    "    percepcao_valor: Optional[list] = Field(description='Extraia qualquer frase sobre o valor ou preço do produto. Retorne uma lista.')\n",
    "\n",
    "# 2️⃣ Inicializa o modelo de chat (GPT-3.5)\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# 3️⃣ Cria parser Pydantic\n",
    "parser = PydanticOutputParser(pydantic_object=AvaliacaoReview)\n",
    "\n",
    "# 4️⃣ Cria prompt\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Analise o seguinte review do cliente e extraia os dados no formato Pydantic:\\n\\n{review}\\n\\n\"\n",
    "    \"Responda estritamente no formato JSON de acordo com o modelo AvaliacaoReview.\"\n",
    ")\n",
    "\n",
    "# 5️⃣ Cria chain\n",
    "chain = LLMChain(\n",
    "    llm=chat,\n",
    "    prompt=prompt,\n",
    "    output_parser=parser\n",
    ")\n",
    "\n",
    "# 6️⃣ Executa chain\n",
    "resposta = chain.run({\"review\": review_cliente})\n",
    "\n",
    "# 7️⃣ Mostra resultado\n",
    "print(resposta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37bb0476",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_cliente = \"\"\"Este soprador de folhas é bastante incrível. Ele tem \n",
    "quatro configurações: sopro de vela, brisa suave, cidade ventosa \n",
    "e tornado. Chegou em dois dias, bem a tempo para o presente de \n",
    "aniversário da minha esposa. Acho que minha esposa gostou tanto \n",
    "que ficou sem palavras. Até agora, fui o único a usá-lo, e tenho \n",
    "usado em todas as manhãs alternadas para limpar as folhas do \n",
    "nosso gramado. É um pouco mais caro do que os outros sopradores \n",
    "de folhas disponíveis no mercado, mas acho que vale a pena pelas \n",
    "características extras.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f11bea4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6eadd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2494a91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class ecomeerce(BaseModel):\n",
    "    \"\"\"Avalição do review do cliente\"\"\"\n",
    "    produto: str = Field(description='Breve descrição do produto')\n",
    "    entrega: bool = Field(description='Cliente ficou satisfeito com a entrega, true ou false.')\n",
    "    avaliacao_produto: bool = Field(description='Cliente ficou satisfeito com o produto, true ou false')\n",
    "    atendimento: bool = Field(description='Cliente ficou satisfeito com o atendimento, true ou false')\n",
    "    satisfacao: Optional[int] = Field(description='Extraia qualquer frase sobre a satisfação geral do cliente com a compra. Retorne uma lista.')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "llm_estrturada = chat.with_structured_output(ecomeerce)\n",
    "resposta = llm_estrturada.invoke(review_cliente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ce3587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ecomeerce(produto='soprador de folhas', entrega=True, avaliacao_produto=True, atendimento=True, satisfacao=8)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "resposta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6119a5",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9718bb44",
   "metadata": {},
   "source": [
    "#### LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5ccc863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "901b3247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fa74db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template('Crie uma frase sobre o seguinte assunto: {assunto}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afd41b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='O futebol é muito mais do que um simples esporte, é a paixão que une milhões de pessoas ao redor do mundo.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 21, 'total_tokens': 52, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-f2059531-f154-45c7-851c-f82e3998978b-0', usage_metadata={'input_tokens': 21, 'output_tokens': 31, 'total_tokens': 52, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model\n",
    "chain.invoke({'assunto': 'futebol'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7211ea8c",
   "metadata": {},
   "source": [
    "##### Adicionando mais elementos a chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b4c4a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'O futebol é muito mais do que um simples jogo, é paixão, emoção e união de povos ao redor do mundo.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "chain.invoke({'assunto': 'futebol'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c20db827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'O Cruzeiro Esporte Clube é mais do que um time, é uma paixão que corre nas veias de todos os seus torcedores.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template('Crie uma frase sobre o seguinte assunto: {assunto}')\n",
    "\n",
    "chain = prompt | ChatOpenAI() | StrOutputParser()\n",
    "chain.invoke({'assunto': 'Cruzeiro Esporte Clube'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d617484",
   "metadata": {},
   "source": [
    "##### ATENÇÃO! A ORDEM IMPORTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3dc4e61f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m chain \u001b[38;5;241m=\u001b[39m  ChatOpenAI() \u001b[38;5;241m|\u001b[39m prompt \u001b[38;5;241m|\u001b[39m StrOutputParser()\n\u001b[1;32m----> 2\u001b[0m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43massunto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCruzeiro Esporte Clube\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Thiago\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3032\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3030\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[0;32m   3031\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 3032\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3033\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3034\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32mc:\\Users\\Thiago\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:370\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    365\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    366\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    369\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[1;32m--> 370\u001b[0m             [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m],\n\u001b[0;32m    371\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    372\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    373\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    374\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    375\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    376\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    377\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    378\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    379\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32mc:\\Users\\Thiago\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:355\u001b[0m, in \u001b[0;36mBaseChatModel._convert_input\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatPromptValue(messages\u001b[38;5;241m=\u001b[39mconvert_to_messages(\u001b[38;5;28minput\u001b[39m))\n\u001b[0;32m    351\u001b[0m msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28minput\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust be a PromptValue, str, or list of BaseMessages.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    354\u001b[0m )\n\u001b[1;32m--> 355\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages."
     ]
    }
   ],
   "source": [
    "chain =  ChatOpenAI() | prompt | StrOutputParser()\n",
    "chain.invoke({'assunto': 'Cruzeiro Esporte Clube'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e38f247",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {'assunto': 'gatinhos'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84abd48d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='Crie uma frase sobre o seguinte assunto: gatinhos', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_formatado = prompt.invoke(input)\n",
    "prompt_formatado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8f88835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Gatinhos são criaturas adoráveis que enchem nossas vidas de alegria e amor com suas brincadeiras e ronronados.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 21, 'total_tokens': 55, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-dd63719a-e0a8-4a80-bcb3-8af2c11bda27-0', usage_metadata={'input_tokens': 21, 'output_tokens': 34, 'total_tokens': 55, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resposta = model.invoke(prompt_formatado)\n",
    "resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f1628e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gatinhos são criaturas adoráveis que enchem nossas vidas de alegria e amor com suas brincadeiras e ronronados.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StrOutputParser().invoke(resposta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e1330c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'properties': {'assunto': {'title': 'Assunto', 'type': 'string'}},\n",
       " 'required': ['assunto'],\n",
       " 'title': 'PromptInput',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.input_schema.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db7ec23e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$defs': {'AIMessage': {'additionalProperties': True,\n",
       "   'description': 'Message from an AI.\\n\\nAIMessage is returned from a chat model as a response to a prompt.\\n\\nThis message represents the output of the model and consists of both\\nthe raw output as returned by the model together standardized fields\\n(e.g., tool calls, usage metadata) added by the LangChain framework.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'ai',\n",
       "     'default': 'ai',\n",
       "     'enum': ['ai'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'example': {'default': False, 'title': 'Example', 'type': 'boolean'},\n",
       "    'tool_calls': {'default': [],\n",
       "     'items': {'$ref': '#/$defs/ToolCall'},\n",
       "     'title': 'Tool Calls',\n",
       "     'type': 'array'},\n",
       "    'invalid_tool_calls': {'default': [],\n",
       "     'items': {'$ref': '#/$defs/InvalidToolCall'},\n",
       "     'title': 'Invalid Tool Calls',\n",
       "     'type': 'array'},\n",
       "    'usage_metadata': {'anyOf': [{'$ref': '#/$defs/UsageMetadata'},\n",
       "      {'type': 'null'}],\n",
       "     'default': None}},\n",
       "   'required': ['content'],\n",
       "   'title': 'AIMessage',\n",
       "   'type': 'object'},\n",
       "  'AIMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'Message chunk from an AI.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'AIMessageChunk',\n",
       "     'default': 'AIMessageChunk',\n",
       "     'enum': ['AIMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'example': {'default': False, 'title': 'Example', 'type': 'boolean'},\n",
       "    'tool_calls': {'default': [],\n",
       "     'items': {'$ref': '#/$defs/ToolCall'},\n",
       "     'title': 'Tool Calls',\n",
       "     'type': 'array'},\n",
       "    'invalid_tool_calls': {'default': [],\n",
       "     'items': {'$ref': '#/$defs/InvalidToolCall'},\n",
       "     'title': 'Invalid Tool Calls',\n",
       "     'type': 'array'},\n",
       "    'usage_metadata': {'anyOf': [{'$ref': '#/$defs/UsageMetadata'},\n",
       "      {'type': 'null'}],\n",
       "     'default': None},\n",
       "    'tool_call_chunks': {'default': [],\n",
       "     'items': {'$ref': '#/$defs/ToolCallChunk'},\n",
       "     'title': 'Tool Call Chunks',\n",
       "     'type': 'array'}},\n",
       "   'required': ['content'],\n",
       "   'title': 'AIMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'ChatMessage': {'additionalProperties': True,\n",
       "   'description': 'Message that can be assigned an arbitrary speaker (i.e. role).',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'chat',\n",
       "     'default': 'chat',\n",
       "     'enum': ['chat'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'role': {'title': 'Role', 'type': 'string'}},\n",
       "   'required': ['content', 'role'],\n",
       "   'title': 'ChatMessage',\n",
       "   'type': 'object'},\n",
       "  'ChatMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'Chat Message chunk.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'ChatMessageChunk',\n",
       "     'default': 'ChatMessageChunk',\n",
       "     'enum': ['ChatMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'role': {'title': 'Role', 'type': 'string'}},\n",
       "   'required': ['content', 'role'],\n",
       "   'title': 'ChatMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'ChatPromptValueConcrete': {'description': 'Chat prompt value which explicitly lists out the message types it accepts.\\n\\nFor use in external schemas.',\n",
       "   'properties': {'messages': {'items': {'oneOf': [{'$ref': '#/$defs/AIMessage'},\n",
       "       {'$ref': '#/$defs/HumanMessage'},\n",
       "       {'$ref': '#/$defs/ChatMessage'},\n",
       "       {'$ref': '#/$defs/SystemMessage'},\n",
       "       {'$ref': '#/$defs/FunctionMessage'},\n",
       "       {'$ref': '#/$defs/ToolMessage'},\n",
       "       {'$ref': '#/$defs/AIMessageChunk'},\n",
       "       {'$ref': '#/$defs/HumanMessageChunk'},\n",
       "       {'$ref': '#/$defs/ChatMessageChunk'},\n",
       "       {'$ref': '#/$defs/SystemMessageChunk'},\n",
       "       {'$ref': '#/$defs/FunctionMessageChunk'},\n",
       "       {'$ref': '#/$defs/ToolMessageChunk'}]},\n",
       "     'title': 'Messages',\n",
       "     'type': 'array'},\n",
       "    'type': {'const': 'ChatPromptValueConcrete',\n",
       "     'default': 'ChatPromptValueConcrete',\n",
       "     'enum': ['ChatPromptValueConcrete'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'}},\n",
       "   'required': ['messages'],\n",
       "   'title': 'ChatPromptValueConcrete',\n",
       "   'type': 'object'},\n",
       "  'FunctionMessage': {'additionalProperties': True,\n",
       "   'description': 'Message for passing the result of executing a tool back to a model.\\n\\nFunctionMessage are an older version of the ToolMessage schema, and\\ndo not contain the tool_call_id field.\\n\\nThe tool_call_id field is used to associate the tool call request with the\\ntool call response. This is useful in situations where a chat model is able\\nto request multiple tool calls in parallel.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'function',\n",
       "     'default': 'function',\n",
       "     'enum': ['function'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'}},\n",
       "   'required': ['content', 'name'],\n",
       "   'title': 'FunctionMessage',\n",
       "   'type': 'object'},\n",
       "  'FunctionMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'Function Message chunk.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'FunctionMessageChunk',\n",
       "     'default': 'FunctionMessageChunk',\n",
       "     'enum': ['FunctionMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'}},\n",
       "   'required': ['content', 'name'],\n",
       "   'title': 'FunctionMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'HumanMessage': {'additionalProperties': True,\n",
       "   'description': 'Message from a human.\\n\\nHumanMessages are messages that are passed in from a human to the model.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import HumanMessage, SystemMessage\\n\\n        messages = [\\n            SystemMessage(\\n                content=\"You are a helpful assistant! Your name is Bob.\"\\n            ),\\n            HumanMessage(\\n                content=\"What is your name?\"\\n            )\\n        ]\\n\\n        # Instantiate a chat model and invoke it with the messages\\n        model = ...\\n        print(model.invoke(messages))',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'human',\n",
       "     'default': 'human',\n",
       "     'enum': ['human'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'example': {'default': False, 'title': 'Example', 'type': 'boolean'}},\n",
       "   'required': ['content'],\n",
       "   'title': 'HumanMessage',\n",
       "   'type': 'object'},\n",
       "  'HumanMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'Human Message chunk.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'HumanMessageChunk',\n",
       "     'default': 'HumanMessageChunk',\n",
       "     'enum': ['HumanMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'example': {'default': False, 'title': 'Example', 'type': 'boolean'}},\n",
       "   'required': ['content'],\n",
       "   'title': 'HumanMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'InputTokenDetails': {'description': 'Breakdown of input token counts.\\n\\nDoes *not* need to sum to full input token count. Does *not* need to have all keys.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \"audio\": 10,\\n            \"cache_creation\": 200,\\n            \"cache_read\": 100,\\n        }\\n\\n.. versionadded:: 0.3.9',\n",
       "   'properties': {'audio': {'title': 'Audio', 'type': 'integer'},\n",
       "    'cache_creation': {'title': 'Cache Creation', 'type': 'integer'},\n",
       "    'cache_read': {'title': 'Cache Read', 'type': 'integer'}},\n",
       "   'title': 'InputTokenDetails',\n",
       "   'type': 'object'},\n",
       "  'InvalidToolCall': {'description': 'Allowance for errors made by LLM.\\n\\nHere we add an `error` key to surface errors made during generation\\n(e.g., invalid JSON arguments.)',\n",
       "   'properties': {'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'title': 'Name'},\n",
       "    'args': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Args'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},\n",
       "    'error': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'title': 'Error'},\n",
       "    'type': {'const': 'invalid_tool_call',\n",
       "     'enum': ['invalid_tool_call'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'}},\n",
       "   'required': ['name', 'args', 'id', 'error'],\n",
       "   'title': 'InvalidToolCall',\n",
       "   'type': 'object'},\n",
       "  'OutputTokenDetails': {'description': 'Breakdown of output token counts.\\n\\nDoes *not* need to sum to full output token count. Does *not* need to have all keys.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \"audio\": 10,\\n            \"reasoning\": 200,\\n        }\\n\\n.. versionadded:: 0.3.9',\n",
       "   'properties': {'audio': {'title': 'Audio', 'type': 'integer'},\n",
       "    'reasoning': {'title': 'Reasoning', 'type': 'integer'}},\n",
       "   'title': 'OutputTokenDetails',\n",
       "   'type': 'object'},\n",
       "  'StringPromptValue': {'description': 'String prompt value.',\n",
       "   'properties': {'text': {'title': 'Text', 'type': 'string'},\n",
       "    'type': {'const': 'StringPromptValue',\n",
       "     'default': 'StringPromptValue',\n",
       "     'enum': ['StringPromptValue'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'}},\n",
       "   'required': ['text'],\n",
       "   'title': 'StringPromptValue',\n",
       "   'type': 'object'},\n",
       "  'SystemMessage': {'additionalProperties': True,\n",
       "   'description': 'Message for priming AI behavior.\\n\\nThe system message is usually passed in as the first of a sequence\\nof input messages.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import HumanMessage, SystemMessage\\n\\n        messages = [\\n            SystemMessage(\\n                content=\"You are a helpful assistant! Your name is Bob.\"\\n            ),\\n            HumanMessage(\\n                content=\"What is your name?\"\\n            )\\n        ]\\n\\n        # Define a chat model and invoke it with the messages\\n        print(model.invoke(messages))',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'system',\n",
       "     'default': 'system',\n",
       "     'enum': ['system'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'}},\n",
       "   'required': ['content'],\n",
       "   'title': 'SystemMessage',\n",
       "   'type': 'object'},\n",
       "  'SystemMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'System Message chunk.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'SystemMessageChunk',\n",
       "     'default': 'SystemMessageChunk',\n",
       "     'enum': ['SystemMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'}},\n",
       "   'required': ['content'],\n",
       "   'title': 'SystemMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'ToolCall': {'description': 'Represents a request to call a tool.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \"name\": \"foo\",\\n            \"args\": {\"a\": 1},\\n            \"id\": \"123\"\\n        }\\n\\n    This represents a request to call the tool named \"foo\" with arguments {\"a\": 1}\\n    and an identifier of \"123\".',\n",
       "   'properties': {'name': {'title': 'Name', 'type': 'string'},\n",
       "    'args': {'title': 'Args', 'type': 'object'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},\n",
       "    'type': {'const': 'tool_call',\n",
       "     'enum': ['tool_call'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'}},\n",
       "   'required': ['name', 'args', 'id'],\n",
       "   'title': 'ToolCall',\n",
       "   'type': 'object'},\n",
       "  'ToolCallChunk': {'description': 'A chunk of a tool call (e.g., as part of a stream).\\n\\nWhen merging ToolCallChunks (e.g., via AIMessageChunk.__add__),\\nall string attributes are concatenated. Chunks are only merged if their\\nvalues of `index` are equal and not None.\\n\\nExample:\\n\\n.. code-block:: python\\n\\n    left_chunks = [ToolCallChunk(name=\"foo\", args=\\'{\"a\":\\', index=0)]\\n    right_chunks = [ToolCallChunk(name=None, args=\\'1}\\', index=0)]\\n\\n    (\\n        AIMessageChunk(content=\"\", tool_call_chunks=left_chunks)\\n        + AIMessageChunk(content=\"\", tool_call_chunks=right_chunks)\\n    ).tool_call_chunks == [ToolCallChunk(name=\\'foo\\', args=\\'{\"a\":1}\\', index=0)]',\n",
       "   'properties': {'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'title': 'Name'},\n",
       "    'args': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Args'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Id'},\n",
       "    'index': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "     'title': 'Index'},\n",
       "    'type': {'const': 'tool_call_chunk',\n",
       "     'enum': ['tool_call_chunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'}},\n",
       "   'required': ['name', 'args', 'id', 'index'],\n",
       "   'title': 'ToolCallChunk',\n",
       "   'type': 'object'},\n",
       "  'ToolMessage': {'additionalProperties': True,\n",
       "   'description': 'Message for passing the result of executing a tool back to a model.\\n\\nToolMessages contain the result of a tool invocation. Typically, the result\\nis encoded inside the `content` field.\\n\\nExample: A ToolMessage representing a result of 42 from a tool call with id\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import ToolMessage\\n\\n        ToolMessage(content=\\'42\\', tool_call_id=\\'call_Jja7J89XsjrOLA5r!MEOW!SL\\')\\n\\n\\nExample: A ToolMessage where only part of the tool output is sent to the model\\n    and the full output is passed in to artifact.\\n\\n    .. versionadded:: 0.2.17\\n\\n    .. code-block:: python\\n\\n        from langchain_core.messages import ToolMessage\\n\\n        tool_output = {\\n            \"stdout\": \"From the graph we can see that the correlation between x and y is ...\",\\n            \"stderr\": None,\\n            \"artifacts\": {\"type\": \"image\", \"base64_data\": \"/9j/4gIcSU...\"},\\n        }\\n\\n        ToolMessage(\\n            content=tool_output[\"stdout\"],\\n            artifact=tool_output,\\n            tool_call_id=\\'call_Jja7J89XsjrOLA5r!MEOW!SL\\',\\n        )\\n\\nThe tool_call_id field is used to associate the tool call request with the\\ntool call response. This is useful in situations where a chat model is able\\nto request multiple tool calls in parallel.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'tool',\n",
       "     'default': 'tool',\n",
       "     'enum': ['tool'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'},\n",
       "    'artifact': {'default': None, 'title': 'Artifact'},\n",
       "    'status': {'default': 'success',\n",
       "     'enum': ['success', 'error'],\n",
       "     'title': 'Status',\n",
       "     'type': 'string'}},\n",
       "   'required': ['content', 'tool_call_id'],\n",
       "   'title': 'ToolMessage',\n",
       "   'type': 'object'},\n",
       "  'ToolMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'Tool Message chunk.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'const': 'ToolMessageChunk',\n",
       "     'default': 'ToolMessageChunk',\n",
       "     'enum': ['ToolMessageChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'},\n",
       "    'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'},\n",
       "    'artifact': {'default': None, 'title': 'Artifact'},\n",
       "    'status': {'default': 'success',\n",
       "     'enum': ['success', 'error'],\n",
       "     'title': 'Status',\n",
       "     'type': 'string'}},\n",
       "   'required': ['content', 'tool_call_id'],\n",
       "   'title': 'ToolMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'UsageMetadata': {'description': 'Usage metadata for a message, such as token counts.\\n\\nThis is a standard representation of token usage that is consistent across models.\\n\\nExample:\\n\\n    .. code-block:: python\\n\\n        {\\n            \"input_tokens\": 350,\\n            \"output_tokens\": 240,\\n            \"total_tokens\": 590,\\n            \"input_token_details\": {\\n                \"audio\": 10,\\n                \"cache_creation\": 200,\\n                \"cache_read\": 100,\\n            },\\n            \"output_token_details\": {\\n                \"audio\": 10,\\n                \"reasoning\": 200,\\n            }\\n        }\\n\\n.. versionchanged:: 0.3.9\\n\\n    Added ``input_token_details`` and ``output_token_details``.',\n",
       "   'properties': {'input_tokens': {'title': 'Input Tokens', 'type': 'integer'},\n",
       "    'output_tokens': {'title': 'Output Tokens', 'type': 'integer'},\n",
       "    'total_tokens': {'title': 'Total Tokens', 'type': 'integer'},\n",
       "    'input_token_details': {'$ref': '#/$defs/InputTokenDetails'},\n",
       "    'output_token_details': {'$ref': '#/$defs/OutputTokenDetails'}},\n",
       "   'required': ['input_tokens', 'output_tokens', 'total_tokens'],\n",
       "   'title': 'UsageMetadata',\n",
       "   'type': 'object'}},\n",
       " 'anyOf': [{'$ref': '#/$defs/StringPromptValue'},\n",
       "  {'$ref': '#/$defs/ChatPromptValueConcrete'}],\n",
       " 'title': 'ChatPromptTemplateOutput'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.output_schema.model_json_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2755d35",
   "metadata": {},
   "source": [
    "##### Como eu teria feito com chains clássicas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8cfcdc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "prompt = ChatPromptTemplate.from_template(\"Crie uma frase sobre o assunto: {assunto}\")\n",
    "output_parser = StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b21696b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thiago\\AppData\\Local\\Temp\\ipykernel_13692\\77858283.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'assunto': 'sol',\n",
       " 'text': 'O sol é a fonte de luz e calor que dá vida e energia a todos os seres vivos da Terra.'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = LLMChain(\n",
    "    llm= model,\n",
    "    prompt= prompt,\n",
    "    output_parser= output_parser\n",
    "\n",
    ")\n",
    "\n",
    "chain.invoke({'assunto': 'sol'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeb7c7e",
   "metadata": {},
   "source": [
    "##### Desafio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a5a33af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Eu te amo.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template('Traduza a seguinte frase para português: {frase}')\n",
    "\n",
    "chain = prompt | ChatOpenAI() | StrOutputParser()\n",
    "chain.invoke({'frase': 'I love you'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97d5ba8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'O LangChain é um framework open-source que simplifica a criação de aplicações baseadas em modelos de linguagem, permitindo a conexão de LLMs com dados externos e fluxos complexos de raciocínio.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template('Resuma o seguinte texto: {texto}')\n",
    "\n",
    "texto = '''\n",
    "O LangChain é um framework open-source que facilita a criação de aplicações baseadas em modelos de linguagem (LLMs). \n",
    "Em vez de apenas enviar uma pergunta para o modelo e receber uma resposta, o LangChain fornece uma estrutura para \n",
    "conectar LLMs a dados externos, ferramentas e fluxos complexos de raciocínio.\n",
    "'''\n",
    "\n",
    "chain = prompt | ChatOpenAI() | StrOutputParser()\n",
    "chain.invoke({'texto': texto})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352e92b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'O texto fala sobre o LangChain, um framework de código aberto projetado para simplificar o desenvolvimento de aplicativos impulsionados por grandes modelos de idiomas (LLMs). Ele permite a conexão dos LLMs com dados externos, ferramentas e fluxos de trabalho de raciocínio complexos, em vez de apenas enviar um comando ao modelo e receber uma resposta.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template('Traduza o seguinte texto para português: {texto}')\n",
    "prompt_2 = ChatPromptTemplate.from_template('Resuma o seguinte texto: {texto}')\n",
    "\n",
    "\n",
    "texto = '''\n",
    "LangChain is an open-source framework designed to simplify the development of applications powered \n",
    "by large language models (LLMs). Instead of just sending a prompt to the model and receiving a response,\n",
    "LangChain provides the structure to connect LLMs with external data, tools, and complex reasoning workflows.\n",
    "'''\n",
    "\n",
    "chain = prompt1 | prompt_2| ChatOpenAI() | StrOutputParser()\n",
    "chain.invoke({'texto': texto})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6655774c",
   "metadata": {},
   "source": [
    "##### Criando Chains mais complexas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6191828",
   "metadata": {},
   "source": [
    "###### Somando chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "123e4568",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template('Fale uma curiosidade sobre o assunto: {assunto}')\n",
    "chain_curiosidade = prompt | ChatOpenAI() | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69093dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template('Crie uma história sobre o seguinte fato curioso: {assunto}')\n",
    "chain_historia = prompt | ChatOpenAI() | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fe5962a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Há muito tempo atrás, em um oceano muito distante, viviam os peixes, os animais mais antigos do planeta. Eles nadavam livremente pelas águas cristalinas, mantendo suas características semelhantes aos seus ancestrais pré-históricos.\\n\\nUm dia, um peixe curioso chamado Nemo decidiu explorar as profundezas do oceano em busca de aventuras. Ele nadou por entre os recifes de corais, passando por cardumes coloridos e criaturas marinhas exóticas. Nemo estava encantado com a beleza e diversidade do mundo subaquático.\\n\\nEnquanto explorava uma caverna escura, Nemo se deparou com um peixe muito sábio e antigo, conhecido como O Grande Ancião. Este peixe era o guardião de todos os segredos e mistérios do oceano, e possuía uma sabedoria incomparável.\\n\\nO Grande Ancião contou a Nemo sobre a história dos peixes, sua longa jornada através dos séculos e as incríveis transformações que haviam enfrentado ao longo do tempo. Ele explicou como os peixes eram os vertebrados mais primitivos do planeta, mantendo suas características únicas e adaptando-se às mudanças do ambiente ao longo dos milênios.\\n\\nNemo ficou fascinado com as histórias do Grande Ancião e percebeu a importância de respeitar e preservar a vida marinha. Ele prometeu proteger os oceanos e suas criaturas, garantindo que as futuras gerações de peixes pudessem desfrutar da mesma beleza e diversidade que ele havia encontrado em sua jornada.\\n\\nAssim, Nemo retornou ao seu cardume com um novo propósito em mente: sensibilizar seus companheiros peixes sobre a importância da preservação dos oceanos e da vida marinha. Juntos, eles se uniram para proteger seu lar e garantir que os peixes continuassem a nadar livremente pelos mares por muitos e muitos anos. E assim, a história dos peixes mais antigos do planeta se tornou uma lenda eterna, transmitida de geração em geração.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = chain_curiosidade | chain_historia \n",
    "chain.invoke({'assunto': 'peixes'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65660144",
   "metadata": {},
   "source": [
    "###### Como entender o que está acontecendo?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fba1d3",
   "metadata": {},
   "source": [
    "Usando o LANGSMITH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2259d4f1",
   "metadata": {},
   "source": [
    "#### Runnables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60dc803",
   "metadata": {},
   "source": [
    "Métodos dos Runnables de LCEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfe5e9d",
   "metadata": {},
   "source": [
    "- Stream: Transmitir de volta fragmentos da resposta\n",
    "- Invoke: Chamar a cadeia com um input\n",
    "- Batch: Chamar a cadeia com uma lista de inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8af2ac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_template('Crie uma frase sobre o assunto: {assunto}')\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dee3f2",
   "metadata": {},
   "source": [
    "##### Invoke"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1a243b",
   "metadata": {},
   "source": [
    "O invoke é o método básico para inserir uma input na cadeia e receber uma resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf5b73cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Cachorrinhos são seres amorosos que nos ensinam diariamente sobre lealdade e companheirismo.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 20, 'total_tokens': 48, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-ce23a34c-08df-425e-b671-3edc929fdfb3-0', usage_metadata={'input_tokens': 20, 'output_tokens': 28, 'total_tokens': 48, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'assunto': 'cachorrinhos'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "124f4bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Os cachorrinhos são como anjos de quatro patas que nos ensinam o verdadeiro significado do amor incondicional.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 20, 'total_tokens': 50, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-48b88a5e-2ee8-4907-aaca-dd03a1a43c54-0', usage_metadata={'input_tokens': 20, 'output_tokens': 30, 'total_tokens': 50, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke('cachorrinhos')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20f51f5",
   "metadata": {},
   "source": [
    "##### Stream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192b5711",
   "metadata": {},
   "source": [
    "Para recebermos uma saída conforme ela é gerada pelo modelo utilizamos o stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e83ca49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cachorrinhos são a prova de que o amor pode vir em forma de quatro patas e um rabo abanando."
     ]
    }
   ],
   "source": [
    "for stream in chain.stream('cachorrinhos'):\n",
    "    print(stream.content, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ad900a",
   "metadata": {},
   "source": [
    "##### Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed642c60",
   "metadata": {},
   "source": [
    "Para fazermos múltiplas requisições em paralelo utilizamos o batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea89a8b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='Os cachorrinhos são os melhores amigos que podemos ter, sempre prontos para nos alegrar e nos fazer companhia.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 20, 'total_tokens': 49, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-0baa08c3-4e59-4921-9081-68974f360b1b-0', usage_metadata={'input_tokens': 20, 'output_tokens': 29, 'total_tokens': 49, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " AIMessage(content='Gatinhos são seres adoráveis que roubam nossos corações com sua fofura e travessuras.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 19, 'total_tokens': 46, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-81ce3c13-3b68-49cd-91d0-07f8d2222d8c-0', usage_metadata={'input_tokens': 19, 'output_tokens': 27, 'total_tokens': 46, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " AIMessage(content='Os cavalinhos de carrossel são uma diversão clássica que encanta crianças e adultos de todas as idades.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 19, 'total_tokens': 48, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b767051b-c9db-4dc3-84fe-e2abc26d0d1e-0', usage_metadata={'input_tokens': 19, 'output_tokens': 29, 'total_tokens': 48, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([{'assunto': 'cachorrinhos'}, {'assunto': 'gatinhos'}, {'assunto': 'cavalinhos'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a63764f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='Cachorrinhos são verdadeiros presentes de amor e alegria em forma de pelagem fofa e lealdade incondicional.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 20, 'total_tokens': 52, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-dae9b517-9945-4bb7-ba0e-2a37fd9ab43a-0', usage_metadata={'input_tokens': 20, 'output_tokens': 32, 'total_tokens': 52, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " AIMessage(content='\"Os gatinhos são seres adoráveis que trazem alegria e amor para nossas vidas.\"', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 19, 'total_tokens': 45, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-3ef1cd2c-f73d-48cb-b44a-00ce0513ec4a-0', usage_metadata={'input_tokens': 19, 'output_tokens': 26, 'total_tokens': 45, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " AIMessage(content='Os cavalinhos de carrossel são a alegria das crianças em parques de diversões.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 19, 'total_tokens': 42, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-ba9a362c-54c7-4634-b413-e34b54af6cea-0', usage_metadata={'input_tokens': 19, 'output_tokens': 23, 'total_tokens': 42, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([{'assunto': 'cachorrinhos'}, {'assunto': 'gatinhos'}, {'assunto': 'cavalinhos'}], config={'max_concurrency': 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227a3a46",
   "metadata": {},
   "source": [
    "##### Runnable Especial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e126ede3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_template(\"Crie um nome para o seguinte produto: {produto}\")\n",
    "\n",
    "chain_nome = prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eef0b72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_template(\"Descreva  cliente potencial para o seguinte produto: {produto}\")\n",
    "\n",
    "chain_clientes = prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cdd33c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Dado o produto com o seguinte nome e seguinte público potencial, desenvolva um anúncio para o produto.\n",
    "Nome do produto: {nome_produto}\n",
    "Público: {publico}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f644004d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nome_produto': 'Copo Resistente FlexiGlass',\n",
       " 'publico': 'Um cliente potencial para um copo inquebrável pode ser alguém que é desastrado ou tem crianças em casa, pois eles podem derrubar o copo com frequência e se preocupar menos com ele quebrando. Além disso, pessoas que praticam esportes ao ar livre, acampam com frequência ou estão sempre em movimento também podem se interessar por um copo inquebrável, pois ele oferece durabilidade e resistência em situações de uso mais extremas. Além disso, pessoas que buscam por produtos sustentáveis e duráveis também podem ser clientes em potencial para um copo inquebrável, pois ele pode substituir copos descartáveis e de vidro que quebram facilmente.'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel = RunnableParallel({'nome_produto': chain_nome, 'publico': chain_clientes})\n",
    "parallel.invoke({'produto': 'Um copo inquebrável'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e22f55f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apresentamos o SuperCopo Invencível, o copo que vai revolucionar a sua experiência! \\n\\nFeito especialmente para os desajeitados, crianças travessas, aventureiros e todos aqueles que buscam durabilidade e resistência em um copo, o SuperCopo Invencível é o seu companheiro ideal para todas as ocasiões.\\n\\nFeito com material ultra resistente, esse copo inquebrável vai te acompanhar em todas as suas aventuras sem se preocupar com quedas ou acidentes. Perfeito para festas, churrascos, acampamentos, piqueniques e muito mais, o SuperCopo Invencível é o copo definitivo para quem busca praticidade e segurança.\\n\\nNão perca tempo e garanta já o seu SuperCopo Invencível e aproveite momentos inesquecíveis sem se preocupar com quebras e acidentes. Experimente a durabilidade e resistência do SuperCopo Invencível e se surpreenda!'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = parallel | prompt | ChatOpenAI() | StrOutputParser()\n",
    "chain.invoke({'produto': 'Um copo inquebrável'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1549e3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Olá, Maria!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def cumprimentar(nome):\n",
    "    return f'Olá, {nome}!'\n",
    "\n",
    "runnable_cumprimentar = RunnableLambda(cumprimentar)\n",
    "\n",
    "resultado = runnable_cumprimentar.invoke('Maria')\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "82619b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'produto': 'Copo inquebrável',\n",
       " 'nome_produto': 'Copo Resistente Forever',\n",
       " 'publico': 'Um cliente potencial para um copo inquebrável pode ser alguém que tem crianças pequenas em casa e está preocupado com a segurança delas ao usar copos convencionais. Outro cliente potencial pode ser alguém que gosta de fazer atividades ao ar livre, como acampar ou fazer piqueniques, e deseja um copo durável que não quebre facilmente. Também pode atrair pessoas que são desajeitadas e costumam derrubar copos com frequência, ou indivíduos que simplesmente procuram por um copo de alta qualidade e resistente para uso diário.'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Dado o produto com o seguinte nome e seguinte público potencial, desenvolva um anúncio para o produto.\n",
    "Nome do produto: {nome_produto}\n",
    "Característica do produto: {produto}\n",
    "Público: {publico}\n",
    "\"\"\")\n",
    "\n",
    "parallel = RunnablePassthrough().assign(**{'nome_produto': chain_nome, 'publico': chain_clientes})\n",
    "chain = parallel | prompt | ChatOpenAI() | StrOutputParser()\n",
    "parallel.invoke({'produto': 'Copo inquebrável'})\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
